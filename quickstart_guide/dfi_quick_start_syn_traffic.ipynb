{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFI Quick Start Guide - Large Synthetic Dataset of 92B Records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through querying a large synthetic traffic dataset\n",
    "in the Data Flow Index from [General System](https://www.generalsystem.com).\n",
    "\n",
    "OpenAPI specification documentation is available at\n",
    "<https://api.dataflowindex.io/docs/api>.\n",
    "\n",
    "Please refer to https://github.com/thegeneralsystem/dfi-client-examples for\n",
    "the most up-to-date companion documentation.\n",
    "\n",
    "Additional resources and help are available at <https://support.generalsystem.com>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install Python modules if they are not already present.\n",
    "!python3 -m pip install requests tabulate sseclient-py pydeck pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules.\n",
    "import json\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "import sseclient\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This tutorial uses PyDeck to visualise the data on a map.\n",
    "# If you want to visualise data, please install PyDeck following the instructions:\n",
    "#     https://deckgl.readthedocs.io/en/latest/installation.html\n",
    "# You do not need a Mapbox API key (skip this step).\n",
    "# You DO need to enable pydeck for Jupyter (follow this step in the guide).\n",
    "import pydeck as pdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First set your API token to access the DFI API.\n",
    "#\n",
    "# Access to the DFI demonstration servers requires an API token, which may be\n",
    "# obtained free of charge by enrolling at <https://eap.generalsystem.com>. Once\n",
    "# enrolled, your API token may be redeemed from <https://tokens.dataflowindex.io/>.\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "api_token = getpass(\"Enter your API token: \")\n",
    "\n",
    "# Set authorisation headers:\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "}\n",
    "base_url = \"https://api.dataflowindex.io\"\n",
    "query_timeout = 60"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this tutorial we will be querying a large synthetic data set\n",
    "\n",
    "This synthetic data set represent traffic moving across London.\n",
    "\n",
    "| total records\t| 92,435,312,835 |\n",
    "| ------------- | -------------- |\n",
    "| distinct uuids | 1,578,544 |\n",
    "| start time | 2022-01-01 00:00:00 |\n",
    "| end time | 2022-08-26 07:12:00 |\n",
    "\n",
    "Bounding box of all data:\n",
    "\n",
    "|      | Longitude  | Latitude |\n",
    "| ---- | ---------- | -------- |\n",
    "| Min  | -0.5120832 | 51.2810883 |\n",
    "| Max  | 0.322123   | 51.6925997 |\n",
    "\n",
    "\n",
    "\n",
    "#### Hardware\n",
    "- The dataset runs on a single server hosted on AWS\n",
    "- The server is storage optimised, with 192GB ram and 2 x 7.5TB NVMe SSD\n",
    "\n",
    "#### Note: this is a shared instance, and you cannot add or delete data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of instances associated with your API token.\n",
    "r = requests.get(f\"{base_url}/instances\", headers=headers, timeout=query_timeout)\n",
    "print(r.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next select the DFI instance you will be accessing.\n",
    "namespace = \"gs_eap_demo\"\n",
    "instance_name = \"eap-1\"\n",
    "params = {\"instance\": f\"{namespace}.{instance_name}\"}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have created a set of interesting polygons that you can use to query the\n",
    "# London Traffic dataset. These include the London Borough areas,\n",
    "# the Congestion Charging Zone area and more.\n",
    "# The code below lists the polygons available:\n",
    "r = requests.get(f\"{base_url}/namespaces/{namespace}/polygons\", headers=headers, timeout=query_timeout)\n",
    "if r.status_code != 200:\n",
    "    print(f\"Status code: {r.status_code}\")\n",
    "    print(f\"Response:\\n{r.text}\")\n",
    "    r.raise_for_status()\n",
    "data = [[polygon[\"name\"], polygon[\"count\"]] for polygon in r.json()[\"polygons\"]]\n",
    "print(tabulate(data, [\"name\", \"vertices\"], tablefmt=\"pretty\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a helper function that allows us to display a polygon on a map\n",
    "def show_polygon(query_polygon: str) -> pdk.Deck:\n",
    "    \"\"\"Visualise a polygon on a map\"\"\"\n",
    "    r = requests.get(\n",
    "        f\"{base_url}/namespaces/{namespace}/polygons/\" + query_polygon, headers=headers, timeout=query_timeout\n",
    "    )\n",
    "    coordinates = r.json()[\"vertices\"]\n",
    "    geo_json = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": [\n",
    "            {\n",
    "                \"type\": \"Feature\",\n",
    "                \"properties\": {},\n",
    "                \"geometry\": {\"coordinates\": [coordinates], \"type\": \"Polygon\"},\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    geo_json_pdk = pdk.Layer(\n",
    "        \"GeoJsonLayer\",\n",
    "        geo_json,\n",
    "        opacity=0.2,\n",
    "        stroked=False,\n",
    "        filled=True,\n",
    "        extruded=False,\n",
    "        wireframe=True,\n",
    "        get_elevation=\"0\",\n",
    "        get_fill_color=\"[255, 255, 0]\",\n",
    "        get_line_color=[255, 255, 255],\n",
    "        pickable=True,\n",
    "    )\n",
    "    view_state = pdk.ViewState(longitude=-0.1, latitude=51.5, zoom=10, min_zoom=5, max_zoom=15, pitch=0, bearing=0)\n",
    "    return pdk.Deck(layers=[geo_json_pdk], initial_view_state=view_state)\n",
    "\n",
    "\n",
    "show_polygon(\"uk_congestion_charge_zone\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducing the streaming API\n",
    "\n",
    "The dataset we are querying contains 92bn records. For that reason, most queries will take longer than 60 seconds to produce results. \n",
    "\n",
    "To avoid our API call to timeout, we use a streaming protocol, which we demonstrate below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the HTTP headers we specify we would like to stream the results.\n",
    "streaming_headers = {\"Authorization\": f\"Bearer {api_token}\", \"accept\": \"text/event-stream\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many records there are inside a polygon.\n",
    "polygon = \"uk_congestion_charge_zone\"\n",
    "time_params = {\n",
    "    \"startTime\": \"2022-01-01T08:00:00.000Z\",\n",
    "    \"endTime\": \"2022-01-01T09:30:00.000Z\",\n",
    "}\n",
    "r = requests.get(\n",
    "    f\"{base_url}/polygon/{namespace}.{polygon}/count\",\n",
    "    params=params | time_params,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "client = sseclient.SSEClient(r)\n",
    "for index, event in enumerate(client.events(), start=1):\n",
    "    print(f\"Message no. {index} of type {event.event}\")\n",
    "    if event.event == \"keepAlive\":\n",
    "        continue\n",
    "    if event.event == \"finish\":\n",
    "        break\n",
    "    if event.event == \"message\":\n",
    "        # We got some data!\n",
    "        results = event.data\n",
    "        continue\n",
    "    print(\"Unexpected event in bagging area\")\n",
    "if len(results) > 0:\n",
    "    print(f\"The polygon '{polygon}' has {results} records in it.\")\n",
    "else:\n",
    "    print(f\"The polygon '{polygon}' does not contain any point\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add helper functions to simplify our code:\n",
    "def receive_stream_count(response: requests.models.Response) -> List[any]:\n",
    "    client = sseclient.SSEClient(response)\n",
    "    results = []\n",
    "    for event in client.events():\n",
    "        if event.event == \"keepAlive\":\n",
    "            continue\n",
    "        if event.event == \"finish\":\n",
    "            break\n",
    "        if event.event == \"message\":\n",
    "            # We got some data!\n",
    "            results = json.loads(event.data)\n",
    "            continue\n",
    "        print(\"Unexpected event in bagging area\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def receive_stream_entities(response: requests.models.Response) -> List[any]:\n",
    "    client = sseclient.SSEClient(response)\n",
    "    results = []\n",
    "    for event in client.events():\n",
    "        if event.event == \"keepAlive\":\n",
    "            continue\n",
    "        if event.event == \"finish\":\n",
    "            break\n",
    "        if event.event == \"message\":\n",
    "            # We got some data!\n",
    "            results += [json.loads(event.data)]\n",
    "            continue\n",
    "        print(\"Unexpected event in bagging area\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def receive_stream_history(response: requests.models.Response) -> List[any]:\n",
    "    client = sseclient.SSEClient(response)\n",
    "    results = []\n",
    "    for event in client.events():\n",
    "        if event.event == \"keepAlive\":\n",
    "            continue\n",
    "        if event.event == \"finish\":\n",
    "            break\n",
    "        if event.event == \"message\":\n",
    "            # We got some data!\n",
    "            results += json.loads(event.data)\n",
    "            continue\n",
    "        print(\"Unexpected event in bagging area\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many unique entities there are inside a polygon.\n",
    "polygon = \"uk_congestion_charge_zone\"\n",
    "time_params = {\n",
    "    \"startTime\": \"2022-01-01T08:00:00.000Z\",\n",
    "    \"endTime\": \"2022-01-01T09:30:00.000Z\",\n",
    "}\n",
    "r = requests.get(\n",
    "    f\"{base_url}/polygon/{namespace}.{polygon}/entities\",\n",
    "    params=params | time_params,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_entities(r)\n",
    "print(f\"We found the following unique entities: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_history(history: List[List[float]]) -> pdk.Deck:\n",
    "    \"\"\"show history on a map\"\"\"\n",
    "    df = pd.DataFrame(history, columns=[\"Longitude\", \"Latitude\"])\n",
    "\n",
    "    history_pdk = pdk.Layer(\n",
    "        \"ScatterplotLayer\",\n",
    "        df,\n",
    "        get_position=[\"Longitude\", \"Latitude\"],\n",
    "        auto_highlight=True,\n",
    "        elevation_scale=500,\n",
    "        pickable=True,\n",
    "        elevation_range=[0, 300],\n",
    "        extruded=True,\n",
    "        filled=True,\n",
    "        opacity=0.8,\n",
    "        radius_scale=6,\n",
    "        radius_min_pixels=1,\n",
    "        radius_max_pixels=100,\n",
    "        line_width_min_pixels=1,\n",
    "        get_fill_color=[255, 0, 0],\n",
    "        get_line_color=[255, 0, 0],\n",
    "        coverage=1,\n",
    "    )\n",
    "    view_state = pdk.ViewState(longitude=-0.1, latitude=51.5, zoom=10, min_zoom=5, max_zoom=15, pitch=0, bearing=0)\n",
    "    r = pdk.Deck(layers=[history_pdk], initial_view_state=view_state)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all records inside a polygon.\n",
    "polygon = \"uk_congestion_charge_zone\"\n",
    "time_params = {\n",
    "    \"startTime\": \"2022-01-01T08:00:00.000Z\",\n",
    "    \"endTime\": \"2022-01-01T09:30:00.000Z\",\n",
    "}\n",
    "r = requests.get(\n",
    "    f\"{base_url}/polygon/{namespace}.{polygon}/history\",\n",
    "    params=params | time_params,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_history(r)\n",
    "print(f\"Points returned: {len(results)}\")\n",
    "history = [[item[\"coordinate\"][0], item[\"coordinate\"][1]] for item in results]\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also query by polygon supplying the list of vertices of the polygon.\n",
    "# Vertices must be listed in counter-clockwise order as mandated in the geoJSON standard.\n",
    "payload = {\"vertices\": [[-1.1, +1.1], [-1.1, -1.1], [+1.1, -1.1], [+1.1, +1.1], [-1.1, +1.1]]}\n",
    "r = requests.post(\n",
    "    f\"{base_url}/polygon/count\",\n",
    "    json=payload,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    params=params,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_count(r)\n",
    "print(f\"Points found: {results}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polygons can be defined, named and stored for later use.\n",
    "# Polygons are used in \"points in polygon\" queries. As polygons definitions may\n",
    "# be large and complex, they can be stored and referred to by name in queries.\n",
    "# A polygon could be, for instance, the boundary of a country and be several MBs in size.\n",
    "# Here we create a new polygon.\n",
    "# Vertices must be listed in counter-clockwise order as mandated in the geoJSON standard.\n",
    "payload = {\n",
    "    \"name\": \"my-first-polygon\",\n",
    "    \"vertices\": [[-1.1, +1.1], [-1.1, -1.1], [+1.1, -1.1], [+1.1, +1.1], [-1.1, +1.1]],\n",
    "}\n",
    "r = requests.post(f\"{base_url}/polygons\", json=payload, headers=headers, timeout=query_timeout)\n",
    "print(f\"Status code: {r.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the polygon.\n",
    "r = requests.get(f\"{base_url}/polygons\", headers=headers, timeout=query_timeout)\n",
    "if r.status_code != 200:\n",
    "    print(f\"Status code: {r.status_code}\")\n",
    "    print(f\"Response:\\n{r.text}\")\n",
    "    r.raise_for_status()\n",
    "\n",
    "data = [[polygon[\"name\"], polygon[\"count\"]] for polygon in r.json()[\"polygons\"]]\n",
    "print(tabulate(data, [\"name\", \"vertices\"], tablefmt=\"pretty\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bounding box queries methods\n",
    "The user supplies a bounding box by listing its top-left, bottom-right vertices. The DFI will find all points (observations) that lie within. We have 3 types of queries:\n",
    "\n",
    "* `count` - Computes how many points lie within the polygon\n",
    "* `points` - Returns the details of the points that lie within the polygon\n",
    "* `entities` - Returns the list of unique sensor ids that lie within the polygon\n",
    "\n",
    "All queries optionally support time ranges and limit the search to include a list of sensor IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"minLng\": -1, \"minLat\": -1, \"maxLng\": 1, \"maxLat\": 1}\n",
    "r = requests.post(\n",
    "    f\"{base_url}/bounding-box/count\",\n",
    "    json=payload,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    params=params,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_count(r)\n",
    "print(f\"The bounding box has {results} records in it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"minLng\": -1, \"minLat\": -1, \"maxLng\": 1, \"maxLat\": 1}\n",
    "r = requests.post(\n",
    "    f\"{base_url}/bounding-box/history\",\n",
    "    json=payload,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    params=params,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_history(r)\n",
    "history = [[item[\"coordinate\"][0], item[\"coordinate\"][1]] for item in results]\n",
    "print(f\"Records found: {len(history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\"minLng\": -1, \"minLat\": -1, \"maxLng\": 1, \"maxLat\": 1}\n",
    "r = requests.post(\n",
    "    f\"{base_url}/bounding-box/entities\",\n",
    "    json=payload,\n",
    "    headers=streaming_headers,\n",
    "    stream=True,\n",
    "    params=params,\n",
    "    timeout=query_timeout,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "results = receive_stream_entities(r)\n",
    "print(f\"We found the following unique entities: {len(results)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
